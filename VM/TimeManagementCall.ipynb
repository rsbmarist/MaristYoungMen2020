{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.28 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\Russell\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Import statements\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "from pandas.io.json import json_normalize\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from dateutil import tz\n",
    "import demoji\n",
    "from collections import Counter\n",
    "import ast\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, precision_score, recall_score, roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix,roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.python.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_zone = tz.gettz('UTC')\n",
    "#to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "# This is the current list the NYSE will be shut down to observe holidays\n",
    "holidayList = [datetime.date(2020, 4, 10), datetime.date(2020, 5, 25), datetime.date(2020, 7, 3), datetime.date(2020, 9, 7), datetime.date(2020, 11, 26), datetime.date(2020, 12, 25),\n",
    "              datetime.date(2021, 1, 1), datetime.date(2021, 1, 18), datetime.date(2021, 2, 15), datetime.date(2021, 4, 2), datetime.date(2021, 5, 31), datetime.date(2021, 7, 5),\n",
    "              datetime.date(2021, 9, 6), datetime.date(2021, 11, 25), datetime.date(2021, 12, 24), datetime.date(2022, 1, 17), datetime.date(2022, 2, 21), datetime.date(2022, 4, 15),\n",
    "              datetime.date(2022, 5, 30), datetime.date(2022, 7, 4), datetime.date(2022, 9, 5), datetime.date(2022, 11, 24), datetime.date(2022, 12, 26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stock Symbols that we will iterate through\n",
    "# Apple = AAPL\n",
    "# Amazon = AMZN\n",
    "# Google = GOOGL\n",
    "# Microsoft = MSFT\n",
    "# Dell = DELL\n",
    "# IBM = IBM\n",
    "# Intel = INTC\n",
    "# HP = HPQ\n",
    "# Facebook = FB\n",
    "# Cisco Systems = CSCO\n",
    "# Oracle = ORCL\n",
    "# HP Enterprise = HPE\n",
    "# Micron Tech = MU\n",
    "# DXC Tech = DXC\n",
    "# Thermo Fisher Scientific = TMO\n",
    "stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "               \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]\n",
    "key = 'AU74VSFGT1S37O4A'\n",
    "# Get the current directory\n",
    "current = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Twits Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_New_Twits(res):\n",
    "    try:\n",
    "        \n",
    "        #\n",
    "        df = (pd.json_normalize(res['messages'])[{'id','body','created_at','entities.sentiment.basic','symbols'}])\n",
    "        \n",
    "        #Reorders the columns\n",
    "        df = df[['id','body','created_at','entities.sentiment.basic','symbols']]\n",
    "        \n",
    "        #Renames the columns\n",
    "        df = df.rename(columns = {'created_at':'created', 'entities.sentiment.basic': 'tag'})\n",
    "        \n",
    "    except:\n",
    "        df = (pd.json_normalize(res['messages'])[{'id','body','created_at','entities.sentiment','symbols'}])\n",
    "               \n",
    "        #Reorders the columns and gets rid of old symbols column\n",
    "        df = df[['id','body','created_at','entities.sentiment', 'symbols']]\n",
    "        \n",
    "        #Renames the columns\n",
    "        df = df.rename(columns = {'created_at':'created', 'entities.sentiment': 'tag'})\n",
    "        \n",
    "\n",
    "    #The following loops reformat the symbols column\n",
    "    #It creates a list of dictionaries, {symbol: \"symbol of company mentioned\", title: \"name of company\"}\n",
    "    dataList=[]\n",
    "    \n",
    "    for index in df['symbols']:\n",
    "        symbolsList = []\n",
    "\n",
    "        for diction in index:\n",
    "\n",
    "            symbolsList.append({'symbol' : diction.get('symbol'), 'title' : diction.get('title')})\n",
    "            \n",
    "        dataList.append(symbolsList)\n",
    "        \n",
    "    df.insert(5, \"newSymbols\" ,dataList)\n",
    "    \n",
    "    df = df[['id','body','created','tag', 'newSymbols']]\n",
    "    \n",
    "    df = remove_emojis(df)\n",
    "\n",
    "    #Replaces the NAN with a string \"none\"\n",
    "    df = df.replace(np.nan, 'none', regex=True)\n",
    "    \n",
    "    #Replaces the string as a datetime variable\n",
    "    dateFormat = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    df['created'] = pd.to_datetime(df['created'], format=dateFormat)\n",
    "    \n",
    "    df = df.sort_values(by = 'id', ascending = False)\n",
    "    \n",
    "    #returns the dataframe in correct format\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This function takes a Stocktwits dataframe and removes the emojis from the twit.\n",
    "#This function then returns a dataframe with an extra column for the twit without emojis\n",
    "def remove_emojis(dataframe):\n",
    "    cleanSentList=[]\n",
    "    \n",
    "    for row in dataframe.body:\n",
    "        cleanSentList.append(demoji.replace(row))\n",
    "        \n",
    "    dataframe.insert(5, 'cleanSents', cleanSentList)\n",
    "    \n",
    "    dataframe = dataframe[['id', 'cleanSents', 'created','tag', 'newSymbols']]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_twits():\n",
    "    \n",
    "    stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "               \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]\n",
    "    \n",
    "    current=os.path()\n",
    "    \n",
    "    for symbol in stockSymbol:\n",
    "\n",
    "        symbolFolder = str(current)+\"\\\\{}folder\".format(symbol)\n",
    "\n",
    "        #selects the file to add to\n",
    "        file = symbolFolder+'\\\\{}_twits.csv'.format(symbol)\n",
    "\n",
    "        historicalTwits = pd.read_csv(file)\n",
    "        newestID = historicalTwits['id'].iloc[0]\n",
    "\n",
    "        url = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json\".format(symbol)\n",
    "        try:\n",
    "            response = requests.get(url, params = {'since' : newestID}).json()\n",
    "\n",
    "        except:\n",
    "            print(\"error getting request\")\n",
    "            try:\n",
    "                response = requests.get(url, params = {'since' : newestID}).json()\n",
    "            except:\n",
    "                print(\"Second error getting request\")\n",
    "                continue\n",
    "        if response['response']['status'] == 429:\n",
    "            print(\"requests exceeded\")\n",
    "            time.sleep(600)\n",
    "            continue\n",
    "\n",
    "        if pd.json_normalize(response['messages']).empty:\n",
    "            time.sleep(15)\n",
    "\n",
    "            time.sleep(15)\n",
    "            continue\n",
    "\n",
    "        tempTwitsDf = collect_New_Twits(response)\n",
    "\n",
    "        newHistoricalTwits = tempTwitsDf.append(historicalTwits)\n",
    "\n",
    "        newHistoricalTwits = newHistoricalTwits.drop_duplicates(subset = 'id')\n",
    "\n",
    "        newHistoricalTwits.to_csv(file, index=False)\n",
    "        time.sleep(15)\n",
    "\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Value Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_daily_api(stockResponse):\n",
    "    newDf = pd.DataFrame(columns =[\"open\", \"high\", \"low\", \"close\", 'volume','time'])\n",
    "    \n",
    "    for times in stockResponse['Time Series (Daily)'].keys():\n",
    "        timestamp = datetime.datetime.strptime(times, '%Y-%m-%d')\n",
    "        ope = stockResponse['Time Series (Daily)'][times]['1. open']\n",
    "        high = stockResponse['Time Series (Daily)'][times]['2. high']\n",
    "        low = stockResponse['Time Series (Daily)'][times]['3. low']\n",
    "        close = stockResponse['Time Series (Daily)'][times]['4. close']\n",
    "        volume = stockResponse['Time Series (Daily)'][times]['5. volume']\n",
    "        \n",
    "        newDf = newDf.append({\"open\":ope, \"high\":high, \"low\":low,\n",
    "                          \"close\":close, \"volume\":volume, \"time\":timestamp}, ignore_index=True)\n",
    "    newDf['time'] = pd.to_datetime(newDf['time'])\n",
    "    \n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stock_market():\n",
    "    \n",
    "    stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "               \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]\n",
    "    key = 'AU74VSFGT1S37O4A'\n",
    "    \n",
    "    current = os.getcwd()\n",
    "    \n",
    "    for symbol in stockSymbol:\n",
    "        dailyUrl = ('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={}&apikey='+key).format(symbol)\n",
    "        stockRes = requests.get(dailyUrl).json()\n",
    "\n",
    "        companyFolder = str(current)+\"/{}folder\".format(symbol)\n",
    "\n",
    "        file = companyFolder+'/{}Daily.csv'.format(symbol)\n",
    "        df=pd.read_csv(file)\n",
    "        df.time = pd.to_datetime(df.time)\n",
    "\n",
    "        stockAPIdf = format_daily_api(stockRes)\n",
    "\n",
    "        ##Stack the dataframes on top of each other without the issue of different columns\n",
    "        newDailyValueDf = stockAPIdf.append(df)\n",
    "\n",
    "\n",
    "        newDailyValueDf['percentChange'] = newDailyValueDf['close'].astype(float).pct_change(periods=-1)\n",
    "        newDailyValueDf['percentVol'] = newDailyValueDf['volume'].astype(float).pct_change(periods=-1)\n",
    "\n",
    "        dataframeValue = newDailyValueDf[{'open', 'high', 'low', 'close', 'volume', 'time', 'percentChange', 'percentVol'}]\n",
    "\n",
    "        dataframeValue['time'] = pd.to_datetime(dataframeValue['time'])\n",
    "\n",
    "        dataframeValue = dataframeValue.sort_values(by= 'time', ascending = False)\n",
    "\n",
    "        dataframeValue = dataframeValue.drop_duplicates(subset='time')\n",
    "\n",
    "        dataframeValue = dataframeValue[{'open', 'high', 'low', 'close', 'volume', 'time', 'percentChange', 'percentVol'}]\n",
    "\n",
    "        dataframeValue.to_csv(file, index=False)\n",
    "        time.sleep(30)\n",
    "        time.sleep(30)\n",
    "\n",
    "    for stock in stockSymbol:\n",
    "\n",
    "        newDf = pd.DataFrame(columns =[\"open\", \"high\", \"low\", \"close\", 'volume','time'])\n",
    "\n",
    "\n",
    "        url = ('https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={}&interval=1min&outputsize=full&apikey='+key).format(stock)\n",
    "        stockRes = requests.get(url).json()\n",
    "\n",
    "        companyFolder = str(current)+\"/{}folder\".format(stock)\n",
    "\n",
    "        file = companyFolder+'/{}Values.csv'.format(stock)\n",
    "        df=pd.read_csv(file)\n",
    "        df.time = pd.to_datetime(df.time)\n",
    "\n",
    "        for times in stockRes['Time Series (1min)'].keys():\n",
    "            timestamp = datetime.datetime.strptime(times, '%Y-%m-%d %H:%M:%S')\n",
    "            ope = stockRes['Time Series (1min)'][times]['1. open']\n",
    "            high = stockRes['Time Series (1min)'][times]['2. high']\n",
    "            low = stockRes['Time Series (1min)'][times]['3. low']\n",
    "            close = stockRes['Time Series (1min)'][times]['4. close']\n",
    "            volume = stockRes['Time Series (1min)'][times]['5. volume']\n",
    "\n",
    "            newDf = newDf.append({\"open\":ope, \"high\":high, \"low\":low, \"close\":close, \"volume\":volume, \"time\":timestamp}, ignore_index=True)\n",
    "\n",
    "        dataframeValue = newDf.append(df)\n",
    "\n",
    "        dataframeValue = dataframeValue[{'open', 'high', 'low', 'close', 'volume', 'time'}]\n",
    "\n",
    "        dataframeValue['time'] = pd.to_datetime(dataframeValue['time'])\n",
    "\n",
    "        dataframeValue = dataframeValue.drop_duplicates(subset='time')\n",
    "\n",
    "        dataframeValue = dataframeValue.sort_values(by = 'time', ascending = False)\n",
    "\n",
    "        dataframeValue.to_csv(file, index=False)\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_predictions():\n",
    "    #re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "\n",
    "    fullDf = pd.DataFrame(columns=['id', 'cleanSents', 'tag', 'created'])\n",
    "    currentFolder = os.getcwd()\n",
    "\n",
    "    #Will need to switch file path to / instead of \\\\ for deployment \n",
    "    for symbol in stockSymbol:\n",
    "        df = pd.read_csv(currentFolder+\"/{}folder/{}_twits.csv\".format(symbol, symbol))[{'id', 'cleanSents', 'tag', 'created'}]\n",
    "        fullDf = fullDf.append(df)\n",
    "    fullDf = fullDf.drop_duplicates('id')\n",
    "    fullDf = fullDf.reset_index()[{'id','cleanSents','tag', 'created'}]\n",
    "\n",
    "    bullish = fullDf[fullDf['tag'] == 'Bullish'].reset_index()[{'id','cleanSents','tag', 'created'}]\n",
    "    bearish = fullDf[fullDf['tag'] == 'Bearish'].reset_index()[{'id','cleanSents','tag', 'created'}]\n",
    "    none = fullDf[fullDf['tag'] == 'none'].reset_index()[{'id','cleanSents','tag', 'created'}]\n",
    "    taggedDf = bullish.append(bearish)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(taggedDf['cleanSents'].values, taggedDf['tag'].values, test_size=0.2)\n",
    "    vect = TfidfVectorizer()\n",
    "\n",
    "    tf_train = vect.fit_transform(X_train)\n",
    "    tf_test = vect.transform(X_test)\n",
    "\n",
    "    NLPmodel = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr', class_weight='balanced').fit(tf_train, y_train)\n",
    "\n",
    "    predictions = NLPmodel.predict(tf_test)\n",
    "\n",
    "    # Report the predctive performance metrics\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "\n",
    "    if not (path.exists(currentFolder+\"nlpDailyPerformance.csv\")):\n",
    "        NLPdailyPerformance = pd.DataFrame(columns = [\"Accuracy\", \"Balanced_Accuracy\", \"Date\"])\n",
    "        NLPdailyPerformance = NLPdailyPerformance.append({\"Accuracy\" : accuracy,\n",
    "                                          \"Balanced_Accuracy\" : balanced_accuracy,\n",
    "                                            \"Date\" : datetime.datetime.today().date()}, ignore_index=True)\n",
    "    else:\n",
    "        NLPdailyPerformance = pd.read_csv(\"nlpDailyPerformance.csv\")\n",
    "        NLPdailyPerformance = NLPdailyPerformance.append({\"Accuracy\" : accuracy,\n",
    "                                          \"Balanced_Accuracy\" : balanced_accuracy,\n",
    "                                            \"Date\" : datetime.datetime.today().date()}, ignore_index=True)\n",
    "    NLPdailyPerformance.to_csv(\"nlpDailyPerformance.csv\", index=False)\n",
    "    make_datasets(NLPmodel, vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_predict(df, NLPmodel, vect):\n",
    "    if len(df) == 0:\n",
    "        return 0\n",
    "    tf_new = vect.transform(df['cleanSents'])\n",
    "    # get probabilities for positive class\n",
    "    probs = NLPmodel.predict_proba(tf_new)[:,1]\n",
    "    preds = NLPmodel.predict(tf_new)\n",
    "    return len(preds[preds=='Bullish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(company, companyDf, newDataDf, lastClose):\n",
    "    currentFolder = os.getcwd()\n",
    "    #Setup Neural Network\n",
    "    #Variables\n",
    "    x=companyDf[{'prePct_traded_vol', 'prePct_close_val', 'percent_bullish', 'pct_twits_volume'}]\n",
    "    y=companyDf['percentChange']\n",
    "    y=np.reshape(y.values, (-1,1))\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_x.fit(x)\n",
    "    scaler_y.fit(y)\n",
    "    xscale=scaler_x.transform(x)\n",
    "    yscale=scaler_y.transform(y)\n",
    "    \n",
    "    #Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xscale, yscale, test_size=.30)\n",
    "    \n",
    "    #Build Neural Network\n",
    "    NNmodel = Sequential()\n",
    "    NNmodel.add(Dense(50, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    NNmodel.add(Dropout(0.5))\n",
    "    NNmodel.add(Dense(100, activation='relu'))\n",
    "    NNmodel.add(Dropout(0.2))\n",
    "    NNmodel.add(Dense(10, activation='relu'))\n",
    "    NNmodel.add(Dropout(0.5))\n",
    "    NNmodel.add(Dense(1, activation='relu'))\n",
    "    NNmodel.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    \n",
    "    #Recall the best validation loss and reduced learning rate and stop training\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20,  \n",
    "                              min_delta=1e-4, mode='min')\n",
    "\n",
    "    stop_alg = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "\n",
    "    history = NNmodel.fit(X_train, y_train, batch_size=50,  verbose=0, validation_split=0.2, epochs=1000, \n",
    "                  callbacks=[stop_alg, reduce_lr])\n",
    "    \n",
    "    #Used to see the loss of both validation and training over the course of training\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(currentFolder+\"/{}folder/{}nnLoss.png\".format(company, company))\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    #Absolute error historgram\n",
    "    pred = scaler_y.inverse_transform(NNmodel.predict(X_test))\n",
    "    actual = scaler_y.inverse_transform(y_test)\n",
    "    #AvgError = plt.hist(np.abs(actual-pred), bins=11)\n",
    "    #AvgError.savefig(currentFolder+\"/{}folder/{}nnAE.png\".format(company, company))\n",
    "    \n",
    "    meanAbsError = np.mean(np.abs(actual-pred))\n",
    "    meanError = np.mean(actual-pred)\n",
    "    stdError = np.std(actual-pred)\n",
    "    \n",
    "    if (path.exists(currentFolder+\"/{}folder/{}nnDailyPerformance.csv\".format(company, company))):\n",
    "        NNdailyPerformance = pd.read_csv(currentFolder+\"/{}folder/{}nnDailyPerformance.csv\".format(company, company))\n",
    "        NNdailyPerformance = NNdailyPerformance.append({\"MeanAbsError\" : meanAbsError,\n",
    "                                                      \"MeanError\" : meanError,\n",
    "                                                      \"StdError\" : stdError,\n",
    "                                                      \"Date\" : datetime.datetime.today().date()}, ignore_index=True)\n",
    "    \n",
    "    else:\n",
    "        NNdailyPerformance = pd.DataFrame(columns = [\"MeanAbsError\", \"MeanError\", \"StdError\", \"Date\"])\n",
    "        NNdailyPerformance = NNdailyPerformance.append({\"MeanAbsError\" : meanAbsError,\n",
    "                                              \"MeanError\" : meanError,\n",
    "                                              \"StdError\" : stdError,\n",
    "                                              \"Date\" : datetime.datetime.today().date()}, ignore_index=True)\n",
    "        \n",
    "    NNdailyPerformance.to_csv(currentFolder+\"/{}folder/{}nnDailyPerformance.csv\".format(company, company), index=False)\n",
    "    \n",
    "    #Compute the newest prediction\n",
    "    \n",
    "    newData = scaler_y.transform(newDataDf)\n",
    "    newPrediction = scaler_y.inverse_transform(NNmodel.predict(newData))[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    diff = lastClose*newPrediction\n",
    "    newClosePred = lastClose+diff\n",
    "    \n",
    "    #df = pd.DataFrame([newPrediction], columns=['prediction'], \n",
    "    #              index=['{}'.format(company)])\n",
    "    #df['positive'] = df['prediction']>0\n",
    "    \n",
    "    df = pd.DataFrame([{'lastClose' : lastClose, 'prediction': round(newClosePred[0], 2)}], \n",
    "                  index=['{} Last Close'.format(company), '{} Prediciton'.format(company)])\n",
    "    df['positive'] = ((df['prediction']-df['lastClose'])>0)\n",
    "    \n",
    "    columns = ('Last Close', 'Prediciton')\n",
    "    y_pos = np.arange(len(columns))\n",
    "    values = [lastClose, round(newClosePred[0], 2)]\n",
    "    \n",
    "    x=np.arange(len(columns))\n",
    "    width = .8\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x, values, width, color =df.positive.map({True: 'g', False:'r'}))\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Stock Value')\n",
    "    ax.set_title('{} Stock Prediction for {}'.format(company, date.today()))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(columns)\n",
    "\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(round(height, 2)),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, -25),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                   fontsize = 20) \n",
    "    \n",
    "    #plt.bar(y_pos, values, align='center', alpha=0.5, color=df.positive.map({True: 'g', False:'r'}))\n",
    "    #plt.xticks(y_pos, columns)\n",
    "    \n",
    "    #plt.text(, lastClose*.5, str(lastClose))\n",
    "    #plt.text(1, newClosePred*.5, str(newClosePred))\n",
    "\n",
    "    #ax = df.plot(kind='bar', color=df.positive.map({True:'g', False:'r'}), alpha=.65) \n",
    "    #x_offset = -0.5\n",
    "    #y_offset = 0.02\n",
    "    #for p in ax.patches:\n",
    "    #    ax.annotate(str(p.get_height()), (p.get_x()*0.6, p.get_height() * .5), fontsize=20)\n",
    "\n",
    "    plt.savefig(currentFolder+\"/{}folder/{}nnPrediction.png\".format(company, company))\n",
    "    plt.close()\n",
    "        \n",
    "    #print(newPrediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function makes the correct dataframes inorder to make a prediction\n",
    "def make_datasets(nlpmodel, vect):\n",
    "    \n",
    "    currentFolder = os.getcwd()\n",
    "\n",
    "    est = pytz.timezone('US/Eastern')\n",
    "\n",
    "    for symbol in stockSymbol:\n",
    "\n",
    "        #Format twits data to prefered times and types.\n",
    "        dfTwits = pd.read_csv(currentFolder+\"/{}folder/{}_twits.csv\".format(symbol, symbol))\n",
    "        dfTwits['created'] = pd.to_datetime(dfTwits['created'])\n",
    "\n",
    "        estList = []\n",
    "        for index, row in dfTwits.iterrows():\n",
    "            estList.append(dfTwits['created'][index].to_pydatetime().astimezone(est))\n",
    "        dfTwits['estTime'] = estList\n",
    "\n",
    "        #Format daily stock market values to preferred \n",
    "        dfDaily = pd.read_csv(currentFolder+\"/{}folder/{}Daily.csv\".format(symbol, symbol))\n",
    "        dfDaily['time'] = pd.to_datetime(dfDaily['time'])\n",
    "\n",
    "        lowest = '2019-12-20'\n",
    "        twits_vol = []\n",
    "        pct_bullish = []\n",
    "        preModeldf = dfDaily[dfDaily['time']>=lowest]\n",
    "        preModeldf = preModeldf.sort_values(by='time')\n",
    "        preModeldf = preModeldf.set_index('time')\n",
    "        \n",
    "        lastClose = preModeldf['close'][-1]\n",
    "\n",
    "        preModeldf['prePct_traded_vol'] = preModeldf['percentVol'].shift(1)\n",
    "        preModeldf['prePct_close_val'] = preModeldf['percentChange'].shift(1)\n",
    "        preModeldf = preModeldf[{'prePct_traded_vol', 'prePct_close_val', 'percentChange'}]\n",
    "\n",
    "\n",
    "        for row in preModeldf.index[:]:\n",
    "            upper = row.to_pydatetime().replace(tzinfo= est) + timedelta(hours = 8, minutes = 30)\n",
    "            lower = row.to_pydatetime().replace(tzinfo = est) + timedelta(days = -1, hours = 8, minutes = 30)\n",
    "            windowTwits = dfTwits[(dfTwits['estTime']>=lower) & (dfTwits['estTime']<=upper)]\n",
    "            volume = len(windowTwits)\n",
    "            twits_vol.append(volume)\n",
    "\n",
    "            if(volume<1):\n",
    "                pct_bullish.append(.5)\n",
    "                volume = 1\n",
    "            else:\n",
    "                bullish_count = len(windowTwits[windowTwits['tag']=='Bullish']) + count_predict(windowTwits[windowTwits['tag']=='none'],\n",
    "                                                                                               nlpmodel,\n",
    "                                                                                               vect)\n",
    "                pct_bullish.append(bullish_count/volume)\n",
    "        preModeldf['percent_bullish'] = pct_bullish\n",
    "        preModeldf['twits_volume'] = twits_vol\n",
    "        modeldf = preModeldf\n",
    "        modeldf['pct_twits_volume'] = modeldf['twits_volume'].pct_change(1)\n",
    "        modeldf = modeldf[{'percentChange', 'prePct_traded_vol', 'prePct_close_val', 'percent_bullish', 'pct_twits_volume'}]\n",
    "        modeldf=modeldf.replace([np.inf, -np.inf], 0.0).dropna()\n",
    "\n",
    "        \n",
    "        #gather newest row for predictions makes a separate dataframe\n",
    "        predictRow = pd.DataFrame( columns = ['prePct_traded_vol', 'prePct_close_val', 'percent_bullish', 'pct_twits_volume'])\n",
    "        dt = date.today()\n",
    "        newUpper= datetime.datetime.combine(dt, datetime.datetime.min.time()).replace(tzinfo=est) + timedelta(hours=8, minutes=30)\n",
    "        newLower=datetime.datetime.combine(dt, datetime.datetime.min.time()).replace(tzinfo=est) + timedelta(days=-1, hours=8, minutes=30)\n",
    "        newWindowTwits = dfTwits[(dfTwits['estTime']>=newLower) & (dfTwits['estTime']<=newUpper)]\n",
    "        newVolume = len(newWindowTwits)\n",
    "        newBullish = .5\n",
    "        if (newVolume <1):\n",
    "            newBullish = .5\n",
    "            newVolume = 1\n",
    "        else:\n",
    "            new_bullish_count = len(newWindowTwits[newWindowTwits['tag']=='Bullish']) + count_predict(newWindowTwits[newWindowTwits['tag']=='none'],\n",
    "                                                                                                     nlpmodel,\n",
    "                                                                                                     vect)\n",
    "            newBullish = new_bullish_count/newVolume\n",
    "        previousPctTradeVol = dfDaily[0:1][\"percentVol\"][0]\n",
    "        previousPctTradeChange = dfDaily[0:1][\"percentChange\"][0]\n",
    "        percentTwitsVolume = (newVolume - volume)/volume\n",
    "\n",
    "        predictRow = predictRow.append(\n",
    "            {'prePct_traded_vol':previousPctTradeVol,\n",
    "             'prePct_close_val':previousPctTradeChange,\n",
    "             'percent_bullish':newBullish,\n",
    "             'pct_twits_volume':percentTwitsVolume}, ignore_index=True)\n",
    "        \n",
    "        #Make predicitons for the current day\n",
    "        make_prediction(symbol, modeldf, predictRow, lastClose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDonutChart(data,symbol,timeSeries, script, images):\n",
    "    getCount = Counter(k['symbol'] for k in data if dict(k).get('symbol'))\n",
    "    symbolCount = dict(getCount)\n",
    "    symbolCount[symbol] = 0\n",
    "    symbolCount = {k: v for k, v in sorted(symbolCount.items(), key=lambda item: item[1], reverse=True)}\n",
    "    fig, ax = plt.subplots(figsize=(11, 10), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "    cnt = 0\n",
    "    data = []\n",
    "    symbols = []\n",
    "    recipe = []\n",
    "    for key in symbolCount.keys():\n",
    "        data.append(symbolCount[key])\n",
    "        symbols.append(key)\n",
    "        recipe.append(key + ' - ' + str(symbolCount[key]) + ' twits')\n",
    "        cnt += 1\n",
    "        if cnt >= 5:\n",
    "            break\n",
    "\n",
    "    if sum(data) < 15:\n",
    "        print('Not enough data for given time frame')\n",
    "        fig.suptitle('Not enough data for given time frame', fontsize=20)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'TopFiveOtherCompanies'+timeSeries+'.png', optimize=True)\n",
    "        #print('saved '+symbol+'TopFiveOtherCompanies'+timeSeries+'.png to '+os.getcwd())\n",
    "        plt.close()\n",
    "        os.chdir(script)\n",
    "        #print('returning to '+os.getcwd())\n",
    "        return\n",
    "\n",
    "    def explode():\n",
    "        try:\n",
    "            exp = (0.1,0,0,0,0)\n",
    "        except:\n",
    "            exp=None\n",
    "        return(exp)\n",
    "\n",
    "    wedges, texts = ax.pie(data, \n",
    "                           explode=explode(), \n",
    "                           shadow=True, wedgeprops=dict(width=0.5), startangle=-40)\n",
    "\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    kw = dict(arrowprops=dict(arrowstyle=\"-\"),\n",
    "              bbox=bbox_props, zorder=0, va=\"center\")\n",
    "\n",
    "    for i, p in enumerate(wedges):\n",
    "        ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "        y = np.sin(np.deg2rad(ang))\n",
    "        x = np.cos(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "        connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "        kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
    "        ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n",
    "                    horizontalalignment=horizontalalignment, **kw)\n",
    "\n",
    "    ax.legend(wedges, symbols,\n",
    "            fontsize='large',\n",
    "            title_fontsize='large',\n",
    "            title=\"Symbols\",\n",
    "            loc=\"center\",\n",
    "            frameon=False)\n",
    "            #bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "    #ax.set_title(\"Top Ten Companies Mentioned in \" + symbol + \" Twits\", fontsize=30, pad=50)\n",
    "\n",
    "    os.chdir(images)\n",
    "    plt.savefig(symbol+'TopFiveOtherCompanies'+timeSeries+'.png', optimize=True)\n",
    "    #print('saved '+symbol+'TopFiveOtherCompanies'+timeSeries+'.png to '+os.getcwd())\n",
    "    plt.close()\n",
    "    os.chdir(script)\n",
    "    #print('returning to '+os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOtherCompanies(fname, days, script, images):\n",
    "    days='all'\n",
    "    to_ignore = fname[:-10]\n",
    "    # set wd to Symbolfolder\n",
    "    symbolFolder = os.path.join(script, to_ignore+'folder')\n",
    "    os.chdir(symbolFolder)\n",
    "    #print('Pulling data from ' +os.getcwd())\n",
    "    df = pd.read_csv(fname)\n",
    "    df['Date'] = pd.to_datetime(df['created']) # save time string as datetime\n",
    "    stock_ds = []\n",
    "    if days == 'lastWeek':\n",
    "        lastweekdate = pd.to_datetime('today').floor('D') - timedelta(7)\n",
    "        df = df[df.Date >= lastweekdate]\n",
    "        for row in df.newSymbols:\n",
    "            lists = ast.literal_eval(row)\n",
    "            for diction in lists:\n",
    "                stock_ds.append(diction)\n",
    "        MakeDonutChart(stock_ds, to_ignore, 'LastWeek', script, images)\n",
    "    elif days == 'lastMonth':\n",
    "        lastmonthdate = pd.to_datetime('today').floor('D') - timedelta(30)\n",
    "        df = df[df.Date >= lastmonthdate]\n",
    "        for row in df.newSymbols:\n",
    "            lists = ast.literal_eval(row)\n",
    "            for diction in lists:\n",
    "                stock_ds.append(diction)\n",
    "        MakeDonutChart(stock_ds, to_ignore, 'LastMonth', script, images)\n",
    "    elif days == 'lastYear':\n",
    "        lastyeardate = pd.to_datetime('today').floor('D') - timedelta(365)\n",
    "        df = df[df.Date >= lastyeardate]\n",
    "        for row in df.newSymbols:\n",
    "            lists = ast.literal_eval(row)\n",
    "            for diction in lists:\n",
    "                stock_ds.append(diction)\n",
    "        MakeDonutChart(stock_ds, to_ignore, 'LastYear', script, images)\n",
    "    else:\n",
    "        for row in df.newSymbols:\n",
    "            lists = ast.literal_eval(row)\n",
    "            for diction in lists:\n",
    "                stock_ds.append(diction)\n",
    "        MakeDonutChart(stock_ds, to_ignore, 'AllTime', script, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeBarChart(tags,symbol,timeSeries, script, images):\n",
    "    data = []\n",
    "    tag = []\n",
    "    for key in tags.keys():\n",
    "        data.append(tags[key])\n",
    "        tag.append(key)\n",
    "    \n",
    "    df = pd.DataFrame({'Tags':tag, 'val':data})\n",
    "    ax = df.plot.barh('Tags', 'val', color=['g', 'r', 'y'], fontsize=15, figsize=(11,10), legend=False)\n",
    "    ax.set_ylabel('Tags', fontsize=20)\n",
    "    for i, v in enumerate(data):\n",
    "        ax.text(v, i, str(v), fontsize=15, fontweight='bold')\n",
    "    \n",
    "    os.chdir(images)\n",
    "    plt.savefig(symbol+'Tags'+timeSeries+'.png', optimize=True)\n",
    "    #print('saved '+symbol+'Tags'+timeSeries+'.png to '+os.getcwd())\n",
    "    plt.close()\n",
    "    os.chdir(script)\n",
    "    #print('returning to '+os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTags(fname, days, script, images):\n",
    "    days='all'\n",
    "    \n",
    "    symbol = fname[:-10]\n",
    "    # set wd to Symbolfolder\n",
    "    symbolFolder = os.path.join(script, symbol+'folder')\n",
    "    os.chdir(symbolFolder)\n",
    "    #print('Pulling data from ' +os.getcwd())\n",
    "    df = pd.read_csv(fname)\n",
    "    df['Date'] = pd.to_datetime(df['created']) # save time string as datetime\n",
    "    tags = {\n",
    "        'Bullish': 0,\n",
    "        'Bearish': 0,\n",
    "        'none': 0\n",
    "    }\n",
    "\n",
    "    if days == 'lastWeek':\n",
    "        lastweekdate = pd.to_datetime('today').floor('D') - timedelta(7)\n",
    "        df = df[df.Date >= lastweekdate]\n",
    "        for row in df.tag:\n",
    "            tags[row] += 1\n",
    "        MakeBarChart(tags,symbol,'LastWeek', script, images)\n",
    "    elif days == 'lastMonth':\n",
    "        lastmonthdate = pd.to_datetime('today').floor('D') - timedelta(30)\n",
    "        df = df[df.Date >= lastmonthdate]\n",
    "        for row in df.tag:\n",
    "            tags[row] += 1\n",
    "        MakeBarChart(tags,symbol,'LastMonth', script, images)\n",
    "    elif days == 'lastYear':\n",
    "        lastyeardate = pd.to_datetime('today').floor('D') - timedelta(365)\n",
    "        df = df[df.Date >= lastyeardate]\n",
    "        for row in df.tag:\n",
    "            tags[row] += 1\n",
    "        MakeBarChart(tags,symbol,'LastYear', script, images)\n",
    "    else:\n",
    "        for row in df.tag:\n",
    "            tags[row] += 1\n",
    "        MakeBarChart(tags,symbol,'AllTime', script, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVolume(fname, days, script, images):\n",
    "    days='all'\n",
    "    \n",
    "    if 'Values' in fname:\n",
    "        symbol = fname[:-10]\n",
    "    else:\n",
    "        symbol = fname[:-9]\n",
    "    symbolFolder = os.path.join(script, symbol+'folder')\n",
    "    os.chdir(symbolFolder)\n",
    "    #print('Pulling data from ' +os.getcwd())\n",
    "    df = pd.read_csv(fname)\n",
    "    df['Date'] = pd.to_datetime(df['time'])\n",
    "    df[\"SMA1\"] = df['close'].rolling(window=25).mean()\n",
    "    df[\"SMA2\"] = df['close'].rolling(window=100).mean()\n",
    "    \n",
    "    if days == 'lastWeek':\n",
    "        lastweekdate = pd.to_datetime('today').floor('D') - timedelta(7)\n",
    "        df = df[df.Date >= lastweekdate]\n",
    "        ax = df.plot('Date', 'volume', figsize=(15,10), fontsize=15)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Volume', fontsize=20)\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'VolumeLastWeek.png', optimize=True)\n",
    "        #print('saved '+symbol+'VolumeLastWeek.png to '+os.getcwd())\n",
    "        plt.close()\n",
    "        os.chdir(symbolFolder)\n",
    "        #print('returning to '+os.getcwd())\n",
    "        ax = df.plot('Date', 'close', figsize=(11,10), fontsize=15)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Price', fontsize=20)\n",
    "        plt.legend()\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'PriceLastWeek.png', optimize=True)\n",
    "        #print('saved '+symbol+'PriceLastWeek.png to '+os.getcwd())\n",
    "        plt.close()\n",
    "        os.chdir(script)\n",
    "        #print('Done - returning to '+os.getcwd())\n",
    "\n",
    "    elif days == 'lastMonth':\n",
    "        lastmonthdate = pd.to_datetime('today').floor('D') - timedelta(30)\n",
    "        df = df[df.Date >= lastmonthdate]\n",
    "        ax = df.plot('Date', 'volume', figsize=(15,10), fontsize=15)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Volume', fontsize=20)\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'VolumeLastMonth.png', optimize=True)\n",
    "        #print('saved '+symbol+'VolumeLastMonth.png to '+os.getcwd())\n",
    "        os.chdir(symbolFolder)\n",
    "        #print('returning to '+os.getcwd())\n",
    "        plt.close()\n",
    "        ax = df.plot('Date', 'close', figsize=(11,10), fontsize=15)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Price', fontsize=20)\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'PriceLastMonth.png', optimize=True)\n",
    "        #print('saved '+symbol+'PriceLastMonth.png to '+os.getcwd())\n",
    "        os.chdir(script)\n",
    "        #print('Done - returning to '+os.getcwd())\n",
    "        plt.close()\n",
    "\n",
    "    elif days == 'lastYear':\n",
    "        lastyeardate = pd.to_datetime('today').floor('D') - timedelta(365)\n",
    "        df = df[df.Date >= lastyeardate]\n",
    "        ax = df.plot('Date', 'volume', figsize=(15,10), fontsize=15)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Volume', fontsize=20)\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'VolumeLastYear.png', optimize=True)\n",
    "        #print('saved '+symbol+'VolumeLastYear.png to '+os.getcwd())\n",
    "        os.chdir(symbolFolder)\n",
    "        #print('returning to '+os.getcwd())\n",
    "        plt.close()\n",
    "        ax = df.plot('Date', 'close', figsize=(11,10), fontsize=15)\n",
    "        plt.plot(df.Date, df['SMA1'], 'g--', label=\"Simple Moving Average - 25 Days\")\n",
    "        plt.plot(df.Date, df['SMA2'], 'r--', label=\"Simple Moving Average - 100 Days\")\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Price', fontsize=20)\n",
    "        plt.legend()\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'PriceLastYear.png', optimize=True)\n",
    "        #print('saved '+symbol+'PriceLastYear.png to '+os.getcwd())\n",
    "        plt.close()\n",
    "        os.chdir(script)\n",
    "        #print('Done - returning to '+os.getcwd())\n",
    "        \n",
    "    else:\n",
    "        ax = df.plot('Date', 'volume', figsize=(15,10), fontsize=15)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Volume', fontsize=20)\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'VolumeAllTime.png', optimize=True)\n",
    "        #print('saved '+symbol+'VolumeAllTime.png to '+os.getcwd())\n",
    "        plt.close()\n",
    "        os.chdir(symbolFolder)\n",
    "        #print('returning to '+os.getcwd())\n",
    "        ax = df.plot('Date', 'close', figsize=(11,10), fontsize=15)\n",
    "        plt.plot(df.Date, df['SMA1'], 'g--', label=\"Simple Moving Average - 25 Days\")\n",
    "        plt.plot(df.Date, df['SMA2'], 'r--', label=\"Simple Moving Average - 100 Days\")\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Price', fontsize=20)\n",
    "        plt.legend()\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.grid(True)\n",
    "        os.chdir(images)\n",
    "        plt.savefig(symbol+'PriceAllTime.png', optimize=True)\n",
    "        #print('saved '+symbol+'PriceAllTime.png to '+os.getcwd())\n",
    "        os.chdir(script)\n",
    "        #print('Done - returning to '+os.getcwd())\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_visualizations():\n",
    "\n",
    "\n",
    "    stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "                   \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]\n",
    "\n",
    "    # initalize relative path directory\n",
    "    script = os.getcwd()\n",
    "    images = os.path.join(script, 'visualization', 'WebsitePNGs')\n",
    "    \n",
    "    for i in stockSymbol:\n",
    "        days='lastWeek'\n",
    "        GetOtherCompanies('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='lastMonth'\n",
    "        GetOtherCompanies('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='lastYear'\n",
    "        GetOtherCompanies('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='all'\n",
    "        GetOtherCompanies('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='lastWeek'\n",
    "        GetVolume('{}Daily.csv'.format(i), days, script, images)\n",
    "        days='lastMonth'\n",
    "        GetVolume('{}Daily.csv'.format(i), days, script, images)\n",
    "        days='lastYear'\n",
    "        GetVolume('{}Daily.csv'.format(i), days, script, images)\n",
    "        days='all'\n",
    "        GetVolume('{}Daily.csv'.format(i), days, script, images)\n",
    "        days='lastWeek'\n",
    "        GetTags('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='lastMonth'\n",
    "        GetTags('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='lastYear'\n",
    "        GetTags('{}_twits.csv'.format(i), days, script, images)\n",
    "        days='all'\n",
    "        GetTags('{}_twits.csv'.format(i), days, script, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the today is Mon-Fri\n",
    "est_zone = tz.gettz('America/New_York')\n",
    "nowEST = datetime.datetime.today()\n",
    "print(nowEST)\n",
    "\n",
    "while True:\n",
    "    \n",
    "\n",
    "    nowEST = datetime.datetime.today()\n",
    "\n",
    "\n",
    "    if (nowEST.weekday() < 5) and (nowEST.date() not in holidayList):\n",
    "        \n",
    "        #in a Stock Market Open day\n",
    "        \n",
    "        if (nowEST.hour == 8 and nowEST.minute < 40 and nowEST.minute > 30):\n",
    "            start_predictions()\n",
    "            #print(\"compute predictions\")\n",
    "            #print(nowEST)\n",
    "            #print(i)\n",
    "            \n",
    "            start_visualizations()\n",
    "            \n",
    "        elif nowEST.hour == 23 and nowEST.minute < 28:\n",
    "            collect_stock_market()\n",
    "            #print(\"Gather stock market values\")\n",
    "            #print(nowEST)\n",
    "            #print(i)\n",
    "            \n",
    "            #Time it takes to gather stock market data ~30 mins\n",
    "            #~15 mins for daily and ~15 for 1 minute intervals\n",
    "            #time.sleep(1800)\n",
    "\n",
    "\n",
    "        \n",
    "    #print(\"Stock Twits data\")\n",
    "    #print(nowEST)\n",
    "    #Time to gather stocktwits data\n",
    "    #time.sleep(450)\n",
    "    gather_twits()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
