{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.29 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\Russell\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Import statements\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from dateutil import tz\n",
    "import demoji\n",
    "from collections import Counter\n",
    "import ast\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "# This is the current list the NYSE will be shut down to observe holidays\n",
    "holidayList = [datetime.datetime(2020, 4, 10), datetime.datetime(2020, 5, 25), datetime.datetime(2020, 7, 3), datetime.datetime(2020, 9, 7), datetime.datetime(2020, 11, 26), datetime.datetime(2020, 12, 25),\n",
    "              datetime.datetime(2021, 1, 1), datetime.datetime(2021, 1, 18), datetime.datetime(2021, 2, 15), datetime.datetime(2021, 4, 2), datetime.datetime(2021, 5, 31), datetime.datetime(2021, 7, 5),\n",
    "              datetime.datetime(2021, 9, 6), datetime.datetime(2021, 11, 25), datetime.datetime(2021, 12, 24), datetime.datetime(2022, 1, 17), datetime.datetime(2022, 2, 21), datetime.datetime(2022, 4, 15),\n",
    "              datetime.datetime(2022, 5, 30), datetime.datetime(2022, 7, 4), datetime.datetime(2022, 9, 5), datetime.datetime(2022, 11, 24), datetime.datetime(2022, 12, 26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stock Symbols that we will iterate through\n",
    "# Apple = AAPL\n",
    "# Amazon = AMZN\n",
    "# Google = GOOGL\n",
    "# Microsoft = MSFT\n",
    "# Dell = DELL\n",
    "# IBM = IBM\n",
    "# Intel = INTC\n",
    "# HP = HPQ\n",
    "# Facebook = FB\n",
    "# Cisco Systems = CSCO\n",
    "# Oracle = ORCL\n",
    "# HP Enterprise = HPE\n",
    "# Micron Tech = MU\n",
    "# DXC Tech = DXC\n",
    "# Thermo Fisher Scientific = TMO\n",
    "stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "               \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]\n",
    "key = 'AU74VSFGT1S37O4A'\n",
    "# Get the current directory\n",
    "current = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Twits Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_New_Twits(res):\n",
    "    try:\n",
    "        \n",
    "        #\n",
    "        df = (pd.json_normalize(res['messages'])[{'id','body','created_at','entities.sentiment.basic','symbols'}])\n",
    "        \n",
    "        #Reorders the columns\n",
    "        df = df[['id','body','created_at','entities.sentiment.basic','symbols']]\n",
    "        \n",
    "        #Renames the columns\n",
    "        df = df.rename(columns = {'created_at':'created', 'entities.sentiment.basic': 'tag'})\n",
    "        \n",
    "    except:\n",
    "        df = (pd.json_normalize(res['messages'])[{'id','body','created_at','entities.sentiment','symbols'}])\n",
    "               \n",
    "        #Reorders the columns and gets rid of old symbols column\n",
    "        df = df[['id','body','created_at','entities.sentiment', 'symbols']]\n",
    "        \n",
    "        #Renames the columns\n",
    "        df = df.rename(columns = {'created_at':'created', 'entities.sentiment': 'tag'})\n",
    "        \n",
    "\n",
    "    #The following loops reformat the symbols column\n",
    "    #It creates a list of dictionaries, {symbol: \"symbol of company mentioned\", title: \"name of company\"}\n",
    "    dataList=[]\n",
    "    \n",
    "    for index in df['symbols']:\n",
    "        symbolsList = []\n",
    "\n",
    "        for diction in index:\n",
    "\n",
    "            symbolsList.append({'symbol' : diction.get('symbol'), 'title' : diction.get('title')})\n",
    "            \n",
    "        dataList.append(symbolsList)\n",
    "        \n",
    "    df.insert(5, \"newSymbols\" ,dataList)\n",
    "    \n",
    "    df = df[['id','body','created','tag', 'newSymbols']]\n",
    "    \n",
    "    df = remove_emojis(df)\n",
    "\n",
    "    #Replaces the NAN with a string \"none\"\n",
    "    df = df.replace(np.nan, 'none', regex=True)\n",
    "    \n",
    "    #Replaces the string as a datetime variable\n",
    "    dateFormat = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    df['created'] = pd.to_datetime(df['created'], format=dateFormat)\n",
    "    \n",
    "    df = df.sort_values(by = 'id', ascending = False)\n",
    "    \n",
    "    #returns the dataframe in correct format\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This function takes a Stocktwits dataframe and removes the emojis from the twit.\n",
    "#This function then returns a dataframe with an extra column for the twit without emojis\n",
    "def remove_emojis(dataframe):\n",
    "    cleanSentList=[]\n",
    "    \n",
    "    for row in dataframe.body:\n",
    "        cleanSentList.append(demoji.replace(row))\n",
    "        \n",
    "    dataframe.insert(5, 'cleanSents', cleanSentList)\n",
    "    \n",
    "    dataframe = dataframe[['id', 'cleanSents', 'created','tag', 'newSymbols']]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_twits():\n",
    "    for symbol in stockSymbol:\n",
    "\n",
    "        symbolFolder = str(current)+\"\\\\{}folder\".format(symbol)\n",
    "\n",
    "        #selects the file to add to\n",
    "        file = symbolFolder+'\\\\{}_twits.csv'.format(symbol)\n",
    "\n",
    "        historicalTwits = pd.read_csv(file)\n",
    "        newestID = historicalTwits['id'].iloc[0]\n",
    "\n",
    "        url = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json\".format(symbol)\n",
    "        try:\n",
    "            response = requests.get(url, params = {'since' : newestID}).json()\n",
    "\n",
    "        except:\n",
    "            print(\"error getting request\")\n",
    "            try:\n",
    "                response = requests.get(url, params = {'since' : newestID}).json()\n",
    "            except:\n",
    "                print(\"Second error getting request\")\n",
    "                continue\n",
    "        if response['response']['status'] == 429:\n",
    "            print(\"requests exceeded\")\n",
    "            time.sleep(600)\n",
    "            continue\n",
    "\n",
    "        if pd.json_normalize(response['messages']).empty:\n",
    "            time.sleep(15)\n",
    "\n",
    "            time.sleep(15)\n",
    "            continue\n",
    "\n",
    "        tempTwitsDf = collect_New_Twits(response)\n",
    "\n",
    "        newHistoricalTwits = tempTwitsDf.append(historicalTwits)\n",
    "\n",
    "        newHistoricalTwits = newHistoricalTwits.drop_duplicates(subset = 'id')\n",
    "\n",
    "        newHistoricalTwits.to_csv(file, index=False)\n",
    "        time.sleep(15)\n",
    "\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Value Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'AU74VSFGT1S37O4A'\n",
    "stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "               \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gets the parent directory of the current directory\n",
    "current = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_daily_api(stockResponse):\n",
    "    newDf = pd.DataFrame(columns =[\"open\", \"high\", \"low\", \"close\", 'volume','time'])\n",
    "    \n",
    "    for times in stockResponse['Time Series (Daily)'].keys():\n",
    "        timestamp = datetime.strptime(times, '%Y-%m-%d')\n",
    "        ope = stockResponse['Time Series (Daily)'][times]['1. open']\n",
    "        high = stockResponse['Time Series (Daily)'][times]['2. high']\n",
    "        low = stockResponse['Time Series (Daily)'][times]['3. low']\n",
    "        close = stockResponse['Time Series (Daily)'][times]['4. close']\n",
    "        volume = stockResponse['Time Series (Daily)'][times]['5. volume']\n",
    "        \n",
    "        newDf = newDf.append({\"open\":ope, \"high\":high, \"low\":low,\n",
    "                          \"close\":close, \"volume\":volume, \"time\":timestamp}, ignore_index=True)\n",
    "    newDf['time'] = pd.to_datetime(newDf['time'])\n",
    "    \n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stock_market():\n",
    "    for symbol in stockSymbol:\n",
    "        dailyUrl = ('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={}&apikey='+key).format(symbol)\n",
    "        stockRes = requests.get(dailyUrl).json()\n",
    "\n",
    "        companyFolder = str(current)+\"\\\\{}folder\".format(symbol)\n",
    "\n",
    "        file = companyFolder+'\\\\{}Daily.csv'.format(symbol)\n",
    "        df=pd.read_csv(file)\n",
    "        df.time = pd.to_datetime(df.time)\n",
    "\n",
    "        stockAPIdf = format_daily_api(stockRes)\n",
    "\n",
    "        ##Stack the dataframes on top of each other without the issue of different columns\n",
    "        newDailyValueDf = stockAPIdf.append(df)\n",
    "\n",
    "\n",
    "        newDailyValueDf['percentChange'] = newDailyValueDf['close'].astype(float).pct_change(periods=-1)\n",
    "        newDailyValueDf['percentVol'] = newDailyValueDf['volume'].astype(float).pct_change(periods=-1)\n",
    "\n",
    "        dataframeValue = newDailyValueDf[{'open', 'high', 'low', 'close', 'volume', 'time', 'percentChange', 'percentVol'}]\n",
    "\n",
    "        dataframeValue['time'] = pd.to_datetime(dataframeValue['time'])\n",
    "\n",
    "        dataframeValue = dataframeValue.sort_values(by= 'time', ascending = False)\n",
    "\n",
    "        dataframeValue = dataframeValue.drop_duplicates(subset='time')\n",
    "\n",
    "        dataframeValue = dataframeValue[{'open', 'high', 'low', 'close', 'volume', 'time', 'percentChange', 'percentVol'}]\n",
    "\n",
    "        dataframeValue.to_csv(file, index=False)\n",
    "        time.sleep(30)\n",
    "        time.sleep(30)\n",
    "\n",
    "    for stock in stockSymbol:\n",
    "\n",
    "        newDf = pd.DataFrame(columns =[\"open\", \"high\", \"low\", \"close\", 'volume','time'])\n",
    "\n",
    "\n",
    "        url = ('https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={}&interval=1min&outputsize=full&apikey='+key).format(stock)\n",
    "        stockRes = requests.get(url).json()\n",
    "\n",
    "        companyFolder = str(current)+\"\\\\{}folder\".format(stock)\n",
    "\n",
    "        file = companyFolder+'\\\\{}Values.csv'.format(stock)\n",
    "        df=pd.read_csv(file)\n",
    "        df.time = pd.to_datetime(df.time)\n",
    "\n",
    "        for times in stockRes['Time Series (1min)'].keys():\n",
    "            timestamp = datetime.strptime(times, '%Y-%m-%d %H:%M:%S')\n",
    "            ope = stockRes['Time Series (1min)'][times]['1. open']\n",
    "            high = stockRes['Time Series (1min)'][times]['2. high']\n",
    "            low = stockRes['Time Series (1min)'][times]['3. low']\n",
    "            close = stockRes['Time Series (1min)'][times]['4. close']\n",
    "            volume = stockRes['Time Series (1min)'][times]['5. volume']\n",
    "\n",
    "            newDf = newDf.append({\"open\":ope, \"high\":high, \"low\":low, \"close\":close, \"volume\":volume, \"time\":timestamp}, ignore_index=True)\n",
    "\n",
    "        dataframeValue = newDf.append(df)\n",
    "\n",
    "        dataframeValue = dataframeValue[{'open', 'high', 'low', 'close', 'volume', 'time'}]\n",
    "\n",
    "        dataframeValue['time'] = pd.to_datetime(dataframeValue['time'])\n",
    "\n",
    "        dataframeValue = dataframeValue.drop_duplicates(subset='time')\n",
    "\n",
    "        dataframeValue = dataframeValue.sort_values(by = 'time', ascending = False)\n",
    "\n",
    "        dataframeValue.to_csv(file, index=False)\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Twits data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cf9c8ea76dd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Stock Twits data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mgather_twits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-0c98d36b3dec>\u001b[0m in \u001b[0;36mgather_twits\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mnewHistoricalTwits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# If the today is Mon-Fri\n",
    "\n",
    "\n",
    "nowUTC = datetime.datetime.today().replace(tzinfo=from_zone)\n",
    "nowEST = nowUTC.astimezone(to_zone)\n",
    "\n",
    "\n",
    "if (nowEST.weekday() < 5) and (nowEST not in holidayList):\n",
    "\n",
    "    if nowEST.hour == 9 and nowEST.minute == 0:\n",
    "        \n",
    "        print(\"compute predictions\")\n",
    "    elif nowEST.hour == 23 and nowEST.minute < 10:\n",
    "        collect_stock_market()\n",
    "        print(\"Gather stock market values\")\n",
    "  \n",
    "    print(\"Stock Twits data\")\n",
    "    gather_twits()\n",
    "\n",
    "else:\n",
    "    print(\"gather Stock Twits data\")\n",
    "    gather_twits()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
