{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.24 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\Russell\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import demoji\n",
    "from collections import Counter\n",
    "import ast\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stock Symbols that we will iterate through\n",
    "# Apple = AAPL\n",
    "# Amazon = AMZN\n",
    "# Google = GOOGL\n",
    "# Microsoft = MSFT\n",
    "# Dell = DELL\n",
    "# IBM = IBM\n",
    "# Intel = INTC\n",
    "# HP = HPQ\n",
    "# Facebook = FB\n",
    "# Cisco Systems = CSCO\n",
    "# Oracle = ORCL\n",
    "# HP Enterprise = HPE\n",
    "# Micron Tech = MU\n",
    "# DXC Tech = DXC\n",
    "# Thermo Fisher Scientific = TMO\n",
    "stockSymbol = [\"AAPL\", \"AMZN\", \"GOOGL\",\"MSFT\", \"DELL\", \"IBM\", \"INTC\", \"HPQ\",\n",
    "               \"FB\", \"CSCO\", \"ORCL\", \"HPE\", \"MU\", \"DXC\", \"TMO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_New_Twits(res):\n",
    "    try:\n",
    "        \n",
    "        #\n",
    "        df = (pd.json_normalize(res['messages'])[{'id','body','created_at','entities.sentiment.basic','symbols'}])\n",
    "        \n",
    "        #Reorders the columns\n",
    "        df = df[['id','body','created_at','entities.sentiment.basic','symbols']]\n",
    "        \n",
    "        #Renames the columns\n",
    "        df = df.rename(columns = {'created_at':'created', 'entities.sentiment.basic': 'tag'})\n",
    "        \n",
    "    except:\n",
    "        df = (pd.json_normalize(res['messages'])[{'id','body','created_at','entities.sentiment','symbols'}])\n",
    "               \n",
    "        #Reorders the columns and gets rid of old symbols column\n",
    "        df = df[['id','body','created_at','entities.sentiment', 'symbols']]\n",
    "        \n",
    "        #Renames the columns\n",
    "        df = df.rename(columns = {'created_at':'created', 'entities.sentiment': 'tag'})\n",
    "        \n",
    "\n",
    "    #The following loops reformat the symbols column\n",
    "    #It creates a list of dictionaries, {symbol: \"symbol of company mentioned\", title: \"name of company\"}\n",
    "    dataList=[]\n",
    "    \n",
    "    for index in df['symbols']:\n",
    "        symbolsList = []\n",
    "\n",
    "        for diction in index:\n",
    "\n",
    "            symbolsList.append({'symbol' : diction.get('symbol'), 'title' : diction.get('title')})\n",
    "            \n",
    "        dataList.append(symbolsList)\n",
    "        \n",
    "    df.insert(5, \"newSymbols\" ,dataList)\n",
    "    \n",
    "    df = df[['id','body','created','tag', 'newSymbols']]\n",
    "    \n",
    "    df = remove_emojis(df)\n",
    "\n",
    "    #Replaces the NAN with a string \"none\"\n",
    "    df = df.replace(np.nan, 'none', regex=True)\n",
    "    \n",
    "    #Replaces the string as a datetime variable\n",
    "    dateFormat = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    df['created'] = pd.to_datetime(df['created'], format=dateFormat)\n",
    "    \n",
    "    df = df.sort_values(by = 'id', ascending = False)\n",
    "    \n",
    "    #returns the dataframe in correct format\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This function takes a Stocktwits dataframe and removes the emojis from the twit.\n",
    "#This function then returns a dataframe with an extra column for the twit without emojis\n",
    "def remove_emojis(dataframe):\n",
    "    cleanSentList=[]\n",
    "    \n",
    "    for row in dataframe.body:\n",
    "        cleanSentList.append(demoji.replace(row))\n",
    "        \n",
    "    dataframe.insert(5, 'cleanSents', cleanSentList)\n",
    "    \n",
    "    dataframe = dataframe[['id', 'cleanSents', 'created','tag', 'newSymbols']]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Russell\\OneDrive - Marist College\\School\\Spring 2020\\DATA 450 Data Capping\\Repository\\VM\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current = os.getcwd()\n",
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "AMZN\n",
      "GOOGL\n",
      "MSFT\n",
      "DELL\n",
      "IBM\n",
      "INTC\n",
      "HPQ\n",
      "FB\n",
      "CSCO\n",
      "ORCL\n",
      "HPE\n",
      "MU\n",
      "DXC\n",
      "TMO\n"
     ]
    }
   ],
   "source": [
    "for symbol in stockSymbol:\n",
    "    print(symbol)\n",
    "    \n",
    "    symbolFolder = str(current)+\"\\\\{}folder\".format(symbol)\n",
    "\n",
    "    #selects the file to add to\n",
    "    file = symbolFolder+'\\\\{}_twits.csv'.format(symbol)\n",
    "\n",
    "    historicalTwits = pd.read_csv(file)\n",
    "    newestID = historicalTwits['id'].iloc[0]\n",
    "\n",
    "    url = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json\".format(symbol)\n",
    "    try:\n",
    "        response = requests.get(url, params = {'since' : newestID}).json()\n",
    "\n",
    "    except:\n",
    "        print(\"error getting request\")\n",
    "        try:\n",
    "            response = requests.get(url, params = {'since' : newestID}).json()\n",
    "        except:\n",
    "            print(\"Second error getting request\")\n",
    "            continue\n",
    "    if response['response']['status'] == 429:\n",
    "        print(\"requests exceeded\")\n",
    "        time.sleep(600)\n",
    "        continue\n",
    "\n",
    "    if pd.json_normalize(response['messages']).empty:\n",
    "        time.sleep(15)\n",
    "        \n",
    "        time.sleep(15)\n",
    "        continue\n",
    "\n",
    "    tempTwitsDf = collect_New_Twits(response)\n",
    "\n",
    "    newHistoricalTwits = tempTwitsDf.append(historicalTwits)\n",
    "\n",
    "    newHistoricalTwits = newHistoricalTwits.drop_duplicates(subset = 'id')\n",
    "\n",
    "    newHistoricalTwits.to_csv(file, index=False)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
